# -*- coding: utf-8 -*-
"""team3-final-p3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1BjG9Q6Ek-THUljPi_dBWWea54PsrswT0
"""

!pip install -U ray==2.38.0 # <- Important version!
 !pip install -U gymnasium

 !pip install -U mujoco

!apt-get install -y \
   libgl1-mesa-dev \
   libgl1-mesa-glx \
   libglew-dev \
   libosmesa6-dev \
   software-properties-common

!apt-get install -y patchelf

!pip install free-mujoco-py
###from here: https://gist.github.com/BuildingAtom/3119ac9c595324c8001a7454f23bf8c8

!pip install -U moderngl
!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1 #for rendering opengl

from __future__ import annotations

import random

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import torch
import torch.nn as nn
from torch.distributions.normal import Normal

import gymnasium as gym

! pip install gputil
! pip install lz4

plt.rcParams["figure.figsize"] = (10, 5)

#mounting to load the xml of the field
from google.colab import drive
drive.mount('/content/drive')

# Specify the path to your checkpoints folder
models_path = '/content/drive/MyDrive/checkpoints/models/'

import numpy as np
from gymnasium.spaces import Discrete, Box
from gymnasium import utils
from gymnasium.envs.mujoco import MujocoEnv
import os


class HockeyEnv(MujocoEnv, utils.EzPickle):
    metadata = {
        "render_modes": ["human", "rgb_array", "depth_array"],
        "render_fps": 50,
    }

    def __init__(self, episode_len=300, **kwargs):
        utils.EzPickle.__init__(self, **kwargs)
        observation_space = Box(low=-np.inf, high=np.inf, shape=(58,), dtype=np.float32)
        MujocoEnv.__init__(
            self,
            os.path.abspath(models_path + "hockey2.xml"),  # Model path
            10,  # Frame skip
            observation_space=observation_space,
            **kwargs,
        )
        self.step_number = 0
        self.episode_len = episode_len
        self.action_space = Discrete(4)  # 0 = up, 1 = down, 2 = left, 3 = right

        self.puck_joint_names = [f"puck_joint_{i}" for i in range(1, 15)]
        self.pucks_knocked_out = {puck_name: False for puck_name in self.puck_joint_names}

    def step(self, action):
        reward = -5.0  # Encourage movement
        # Map discrete action to forces
        forces = {0: [0, 1], 1: [0, -1], 2: [-1, 0], 3: [1, 0]}
        scaled_force = np.array(forces[action]) * 10
        self.do_simulation(scaled_force, self.frame_skip)
        self.step_number += 1

        # Check for knocked-out pucks
        pos_min, pos_max = -1.0, 1.0
        for puck_name in self.puck_joint_names:
            x, y = self.data.joint(puck_name).qpos[:2]
            if (x < pos_min or x > pos_max or y < pos_min or y > pos_max) and not self.pucks_knocked_out[puck_name]:
                self.pucks_knocked_out[puck_name] = True
                reward += 1000

        # Penalty for player going out of bounds
        player_x = self.data.qpos[self.model.joint("player_x").qposadr]
        player_y = self.data.qpos[self.model.joint("player_y").qposadr]
        if player_x > pos_max or player_x < pos_min or player_y > pos_max or player_y < pos_min:
            reward -= 5000
            done = True
        else:
            done = False

        truncated = self.step_number >= self.episode_len
        obs = self._get_obs()
        return obs, reward, done, truncated, {}

    def _randomize_puck_positions(self):
        pos_min, pos_max = -0.5, 0.5
        for puck_name in self.puck_joint_names:
            x = self.np_random.uniform(low=pos_min, high=pos_max)
            y = self.np_random.uniform(low=pos_min, high=pos_max)
            z = 0.03
            try:
                self.data.joint(puck_name).qpos[:3] = [x, y, z]
            except KeyError:
                print(f"Warning: {puck_name} not found in the model.")

    def reset_model(self):
        self.step_number = 0
        qpos = self.init_qpos + self.np_random.uniform(size=self.model.nq, low=-0.01, high=0.01)
        qvel = self.init_qvel + self.np_random.uniform(size=self.model.nv, low=-0.01, high=0.01)
        self.set_state(qpos, qvel)
        self._randomize_puck_positions()
        for puck_name in self.puck_joint_names:
            self.pucks_knocked_out[puck_name] = False
        return self._get_obs()

    def _get_obs(self):
        obs = np.concatenate(
            (self.data.joint("player_x").qpos[:3], self.data.joint("player_y").qpos[:3])
        )
        for puck_name in self.puck_joint_names:
            puck_position = self.data.joint(puck_name).qpos[:2]
            puck_velocity = self.data.joint(puck_name).qvel[:2]
            obs = np.concatenate((obs, puck_position, puck_velocity))
        return obs

len(env._get_obs())

np.concatenate(np.array([[0,1]]))



import matplotlib.pyplot as plt
from IPython.display import Image, display
import imageio
import numpy as np

# Initialize the environment
env = HockeyEnv(render_mode="rgb_array", camera_name="topdown")

# Action sequences for testing
go_right_then_left = [3] * 10 + [0] * 10 + [2] * 10 + [2] * 10

# Function to add text overlay to frames
def add_text_to_frame(frame, text, x, y):
    fig, ax = plt.subplots()
    ax.imshow(frame)
    ax.axis("off")
    ax.text(x, y, text, color="yellow", fontsize=14, fontweight="bold", backgroundcolor="black")
    fig.canvas.draw()
    # Convert the Matplotlib figure back to an image array
    frame_with_text = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)
    frame_with_text = frame_with_text.reshape(fig.canvas.get_width_height()[::-1] + (4,))
    plt.close(fig)  # Close the figure to save memory
    return frame_with_text

# Function to replay a sequence of actions in the environment and generate frames
def rerun_best_episode(env, best_actions):
    obs, info = env.reset()
    frames = []
    total_reward = 0
    for action in best_actions:
        obs, reward, terminated, truncated, ddd = env.step(action)
        total_reward += reward
        frame = env.render()
        frame_with_text = add_text_to_frame(frame, f"Reward: {total_reward:.2f}", 10, 25)
        frames.append(frame_with_text)
    env.close()
    return frames

# Replay the episode and save the output as a GIF
frames = rerun_best_episode(env, go_right_then_left)
imageio.mimsave("manual.gif", frames, fps=20, loop=0)
display(Image(filename="manual.gif"))



from ray.rllib.algorithms.ppo import PPOConfig
from ray import tune
import os
import matplotlib.pyplot as plt

num_iterations = 250
batch_size = 256 * 8
lambda_ = 0.9
checkpoint_path = '/content/drive/MyDrive/checkpoints/models/h2check2-part3'
best_checkpoint_dir = '/content/drive/MyDrive/checkpoints/models/h2currentbest2-part2'

tune.register_env("my_env", lambda config: HockeyEnv())

config = PPOConfig().environment(env="my_env").training(
    train_batch_size=batch_size,
    lambda_=lambda_,
)
algo = config.build()

from google.colab import drive
drive.mount('/content/drive')
if os.path.exists('/content/drive/MyDrive/checkpoints/models/h2check3'):
    algo.restore('/content/drive/MyDrive/checkpoints/models/h2check3')

os.makedirs(checkpoint_path, exist_ok=True)
os.makedirs(best_checkpoint_dir, exist_ok=True)

best_mean_reward = float('-inf')
kl_divergence_history = []
reward_mean_history = []
reward_min_history = []
reward_max_history = []

# Training loop
for i in range(num_iterations):
    result = algo.train()

    # Track metrics
    kl_divergence = result.get("info", {}).get("learner", {}).get("default_policy", {}).get("learner_stats", {}).get("kl", None)
    reward_mean = result.get("env_runners", {}).get("episode_reward_mean", None)
    reward_min = result.get("env_runners", {}).get("episode_reward_min", None)
    reward_max = result.get("env_runners", {}).get("episode_reward_max", None)

    if reward_mean and reward_mean > best_mean_reward:
        best_mean_reward = reward_mean
        best_checkpoint_path = algo.save(best_checkpoint_dir)
        print(f"New best model saved with mean reward: {best_mean_reward:.2f} at {best_checkpoint_path}")

    # Append metrics
    if kl_divergence is not None:
        kl_divergence_history.append(kl_divergence)
    if reward_mean is not None:
        reward_mean_history.append(reward_mean)
    if reward_min is not None:
        reward_min_history.append(reward_min)
    if reward_max is not None:
        reward_max_history.append(reward_max)

    # Save progress every 10 iterations
    if i % 10 == 0:
        algo.save(checkpoint_path)
        print(f"Checkpoint saved at iteration {i}")

        # Plot KL divergence and rewards
        if i > 0:
            fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 10))
            fig.suptitle("Training Progress")

            # KL Divergence Plot
            ax1.plot(kl_divergence_history, label="KL Divergence")
            ax1.set_xlabel("Iteration")
            ax1.set_ylabel("KL Divergence")
            ax1.legend()

            # Reward Plot (mean, min, max)
            ax2.plot(reward_mean_history, label="Episode Reward Mean", color="blue")
            ax2.plot(reward_min_history, label="Episode Reward Min", color="red")
            ax2.plot(reward_max_history, label="Episode Reward Max", color="green")
            ax2.set_xlabel("Iteration")
            ax2.set_ylabel("Reward")
            ax2.legend()
            plt.show()

# Save final checkpoint
final_checkpoint_dir = algo.save(checkpoint_path)
print(f"Final checkpoint saved in directory {final_checkpoint_dir}")

#SKIP THIS CELL to just use the last iteration of algo from above
### To restore the checkpointed training
#
#from ray import tune
#import gymnasium as gym
#import matplotlib.pyplot as plt
#from IPython.display import Image, display
#import imageio
#import time
#
#
#tune.register_env("my_env", lambda config: HockeyEnv())
#
# Clean up any previous algorithm instance
#if 'algo' in globals():
#    algo.stop()
#    del algo
#
#checkpoint_path = '/content/drive/MyDrive/checkpoints/models/h2check2'
#restore_me = checkpoint_path# '/content/drive/MyDrive/checkpoints/models/check3'
#from ray.rllib.algorithms.ppo import PPOConfig
#config = PPOConfig().environment(env="my_env")#.training(train_batch_size=512)
#config.model.update(model_config)
# config.resources(num_gpus=.05)
# Build.
#algo = config.build()
#algo.restore(restore_me)

## Tests the policy in algo

example_runs = 10
prepend_to_gif_file = "trial-"


#Testing "algo" and demonstrating its use with GIFs
policy = algo.get_policy()

import matplotlib.pyplot as plt
from IPython.display import Image, display
import imageio

env = HockeyEnv(render_mode="rgb_array",camera_name="topdown")


def add_text_to_frame(frame, text, x, y):
    # Convert the frame to a Matplotlib figure with the text overlay
    fig, ax = plt.subplots()
    ax.imshow(frame)
    ax.axis('off')  # Hide axes
    ax.text(x, y, text, color="yellow", fontsize=14, fontweight='bold', backgroundcolor='black')
    fig.canvas.draw()
    # Convert the Matplotlib figure back to an image array
    frame_with_text = np.frombuffer(fig.canvas.buffer_rgba(), dtype=np.uint8)
    frame_with_text = frame_with_text.reshape(fig.canvas.get_width_height()[::-1] + (4,))  # RGBA channels
    plt.close(fig)  # Close the figure to save memory
    return frame_with_text

def render_policy_episode(env, extra_frames=200):
    env.reset()
    steps = 0
    step_counter = 0
    keep_rendering = True
    obs, info = env.reset()
    frames = []
    total_reward = 0
    while keep_rendering:
        steps += 1
        action = policy.compute_single_action(obs)[0]
        obs, reward, terminated, truncated, ddd = env.step(action)
        total_reward += reward
        frame = env.render()  # Assuming "rgb_array" mode
        frame_with_text = add_text_to_frame(frame, "step: "+str(steps)+ f" Reward: {total_reward:.2f} ", 10, 25)
        frames.append(frame_with_text)
        # Check for done or truncated flags and start counting extra frames
        if terminated or truncated:
            step_counter += 1
            if step_counter >= extra_frames:
                keep_rendering = False
                break
        else:
            step_counter = 0  # Reset the counter if not done/truncated
    return frames

for i in range(example_runs):
    frames = render_policy_episode(env,300)
    savefilename = prepend_to_gif_file+str(i)+'.gif'
    imageio.mimsave(savefilename, frames, fps=20, loop=0)#0 goes forever
    print("Saved "+savefilename)
    display(Image(filename=savefilename))

"""# **Reflections:**

## **Summary of RL Training and Changes**

### **Note**:
Agent was trained for **250 iterations**, which took approximately **1 hour** to train each run.

### **Environment Setup**:
- Player starts at the center of the field and is surrounded by 15 pucks.
- The observation space tracks the player’s position, velocity, and the positions/velocities of all of the pucks.
- The action space uses discrete actions mapped to directional forces.

### **Reward System**:
- **+1000**: Reward for each puck knocked off the field.
- **-5**: Penalty for unnecessary movements.
- **-5000**: Penalty for going out of bounds.

### **Training Details**:
- The agent was trained using **250 iterations**.
- Mean, min, max, and KL divergence were logged and visualized for monitoring the learning progress.

**SEE**: SLIDE 7 in “project 3 - group 3” PowerPoint attachment.

- Checkpoints were saved periodically to preserve progress and allow re-training or evaluation of earlier models (also if it timed out or Wi-Fi cut out).

### **Evaluation Process**:
- After training, the agent’s behavior was evaluated by replaying its actions using the trained policy and GIFs were created to visualize the agent’s performance.

**SEE**: SLIDE 7 in “project 3 - group 3” PowerPoint attachment.

### **Key Observations**:
- The agent started with random movements, hitting some pucks but gradually learned to focus on the nearest puck in bounds and move towards it.

**SEE**: SLIDES 2 & 3 in “project 3 - group 3” PowerPoint attachment.

- By the end of training, it was able to clear multiple pucks in succession while staying within bounds for the most part. It will correct itself to get the pucks that have not been cleared.

**SEE**: SLIDE 4 in “project 3 - group 3” PowerPoint attachment.

---

## **Reflection on Successes and Novel Behavior**

### **Early Training**:
At the start, the player moved randomly and often didn’t interact with the pucks. Some pucks moved on their own at the beginning of trials, which inflated rewards without the player actually doing anything. The player also went out of bounds frequently, leading to large penalties.

**SEE**: SLIDE 2 in “project 3 - group 3” PowerPoint attachment.

### **Mid Training**:
Around **100 iterations**, the player started targeting nearby pucks but often overshot the testing area, earning penalties. Movements were inefficient, with unnecessary backtracking after pushing pucks.

**SEE**: SLIDE 3 in “project 3 - group 3” PowerPoint attachment.

### **Late Training**:
By the end of **250 iterations**, the player showed a clear strategy: pushing the nearest puck off the field before moving to the next. Overshooting became rare, and movements were smoother. However, some pucks still moved without contact early in the trials.

**SEE**: SLIDES 4 & 6 in “project 3 - group 3” PowerPoint attachment.

### **Failures and Challenges**:
- Early random behavior made learning slow.
- Pucks moving on their own at the start gave unearned rewards.
- Overshooting the field delayed progress and caused penalties.

**SEE**: SLIDE 5 in “project 3 - group 3” PowerPoint attachment.

---

## **Reflection on Key Changes**

### **Most Impactful Changes**
1. **Reward Adjustments**:
   - Adding a penalty for idle movement encouraged the player to stay active and explore the field.
   - A large penalty for going out of bounds (-5000) also reduced wasted effort and kept the player within the field.

2. **Simplified Observation Space**:
   - Instead of tracking all puck positions and velocities, focusing on the nearest puck made learning faster.

3. **Batch Size and Iterations**:
   - Using a larger batch size (256×8) helped stabilize training, even with only 250 iterations.

### **What We Would Do Differently**:
1. **Fix Environmental Issues**:
   - Address the issue where some pucks move at the start of the trial without being touched, as it skews rewards and evaluation.

2. **Explore Continuous Action Space**:
   - Switching to continuous actions could allow for smoother and more precise movements, potentially leading to faster learning.

3. **Extend Training**:
   - Run for more than 250 iterations if computational resources allow and time permits.

---

## **Most Successful Example**

### **Training Conditions That Led to Success**:
Same as reflection question 1.

**SEE**: SLIDES 6 & 7 in “project 3 - group 3” PowerPoint attachment.


"""